{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Export and Optimization Pipeline\n",
    "\n",
    "This notebook exports and quantises a pre-trained sentiment analysis model through the following steps:\n",
    "\n",
    "1. Loading a PyTorch model and tokeniser\n",
    "2. Converting to ONNX Runtime format for improved inference performance\n",
    "3. Applying quantization for model optimization\n",
    "4. Comparing inference speed and outputs between:\n",
    "   - Original PyTorch model\n",
    "   - ONNX Runtime model \n",
    "   - Quantized model\n",
    "\n",
    "The notebook includes a Gradio interface for interactive testing of all three model variants."
   ],
   "id": "5880338f8353c037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from time import perf_counter"
   ],
   "id": "f07cfd4e0cebbb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"models/best/\"\n",
    "\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "tokeniser.save_pretrained(\"models/tokeniser/\")"
   ],
   "id": "751f435144e65061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    export=True,\n",
    "    provider=\"CPUExecutionProvider\"\n",
    ")\n",
    "ort_model.save_pretrained(\"models/ort/\")"
   ],
   "id": "888241303ab107a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "quantized_model_path = \"models/quantized_model/\"\n",
    "quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
    "\n",
    "quantizer.quantize(save_dir=quantized_model_path, quantization_config=quantization_config)\n",
    "\n",
    "quantized_model = ORTModelForSequenceClassification.from_pretrained(quantized_model_path, local_files_only=True)"
   ],
   "id": "9a30fbc0bae9c249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_model(text_in: str, model: any) -> tuple:\n",
    "    tokenised_text = tokeniser(\n",
    "        text_in,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        out = model(**tokenised_text)\n",
    "        execution_time = round(perf_counter() - start_time, 5)\n",
    "        out = round(out.logits.squeeze().item(), 5)\n",
    "\n",
    "    return out, execution_time"
   ],
   "id": "1f7259a884c57524",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compare_models(text_in: str) -> pd.DataFrame:\n",
    "    data = [\n",
    "        [\"PyTorch\", *run_model(text_in, pytorch_model)],\n",
    "        [\"ONNX\", *run_model(text_in, ort_model)],\n",
    "        [\"AutoQuantization\", *run_model(text_in, quantized_model)]\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Model\", \"Output\", \"Time\"])"
   ],
   "id": "eec728976bfade15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The ONNX model achieves nearly identical performance to the quantised version, with only a minimal average difference of ~0.03 in predictions. Given this negligible accuracy impact, the ONNX model is the optimal choice as it provides the best balance between performance and prediction quality.\n",
   "id": "b6534bac163090ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T10:13:10.409796Z",
     "start_time": "2025-08-12T10:13:10.255372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text = gr.Textbox(placeholder=\"Paste a headline here.\")\n",
    "            run_button = gr.Button(\"Run\")\n",
    "        output_table = gr.DataFrame()\n",
    "\n",
    "    run_button.click(fn=compare_models, inputs=text, outputs=output_table)\n",
    "\n",
    "demo.launch()"
   ],
   "id": "4ed5268ae84c454e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
