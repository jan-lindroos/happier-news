{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Export and Optimization Pipeline\n",
    "\n",
    "This notebook exports and quantises a pre-trained sentiment analysis model through the following steps:\n",
    "\n",
    "1. Loading a PyTorch model and tokeniser\n",
    "2. Converting to ONNX Runtime format for improved inference performance\n",
    "3. Applying quantization for model optimization\n",
    "4. Comparing inference speed and outputs between:\n",
    "   - Original PyTorch model\n",
    "   - ONNX Runtime model \n",
    "   - Quantized model\n",
    "\n",
    "The notebook includes a Gradio interface for interactive testing of all three model variants."
   ],
   "id": "5880338f8353c037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from time import perf_counter"
   ],
   "id": "f07cfd4e0cebbb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"models/best/\"\n",
    "\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "tokeniser.save_pretrained(\"models/tokeniser/\")"
   ],
   "id": "751f435144e65061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    export=True,\n",
    "    provider=\"CPUExecutionProvider\"\n",
    ")\n",
    "ort_model.save_pretrained(\"models/ort/\")"
   ],
   "id": "888241303ab107a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "quantized_model_path = \"models/quantized_model/\"\n",
    "quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
    "\n",
    "quantizer.quantize(save_dir=quantized_model_path, quantization_config=quantization_config)\n",
    "\n",
    "quantized_model = ORTModelForSequenceClassification.from_pretrained(quantized_model_path, local_files_only=True)"
   ],
   "id": "9a30fbc0bae9c249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_model(text_in: str, model: any) -> tuple:\n",
    "    \"\"\"\n",
    "    Processes input text through a machine learning model and returns the output and execution time.\n",
    "\n",
    "    This function tokenises the input text using a tokeniser, processes the tokenised output through\n",
    "    the provided model, and calculates the time taken for model inference. The output logits of the\n",
    "    model are then rounded and returned alongside the execution time.\n",
    "\n",
    "    :param text_in: Input text that needs to be processed by the model.\n",
    "    :type text_in: str\n",
    "    :param model: A pre-trained machine learning model capable of inference.\n",
    "    :type model: any\n",
    "    :return: A tuple containing the processed output and the time taken for execution.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    tokenised_text = tokeniser(\n",
    "        text_in,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        out = model(**tokenised_text)\n",
    "        execution_time = round(perf_counter() - start_time, 5)\n",
    "        out = round(out.logits.squeeze().item(), 5)\n",
    "\n",
    "    return out, execution_time"
   ],
   "id": "1f7259a884c57524",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compare_models(text_in: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compares the performance of multiple models by processing the input text through\n",
    "    each model and returning a DataFrame with the results. Each model's output and\n",
    "    execution time are recorded and organised for analysis.\n",
    "\n",
    "    :param text_in: The text input to be processed by each model\n",
    "    :type text_in: str\n",
    "    :return: A pandas DataFrame containing the model name, its output, and the\n",
    "        corresponding execution time\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        [\"PyTorch\", *run_model(text_in, pytorch_model)],\n",
    "        [\"ONNX\", *run_model(text_in, ort_model)],\n",
    "        [\"AutoQuantization\", *run_model(text_in, quantized_model)]\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"Model\", \"Output\", \"Time\"])"
   ],
   "id": "eec728976bfade15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Selection\n",
    "\n",
    "..."
   ],
   "id": "b6534bac163090ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            text = gr.Textbox(placeholder=\"Paste a headline here.\")\n",
    "            run_button = gr.Button(\"Run\")\n",
    "        output_table = gr.DataFrame()\n",
    "\n",
    "    run_button.click(fn=compare_models, inputs=text, outputs=output_table)\n",
    "\n",
    "demo.launch()"
   ],
   "id": "4ed5268ae84c454e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
